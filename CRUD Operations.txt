Elasticsearch CRUD Operation:-

1. Create indices(index) : -
    Indices are created using PUT method:-
     1.curl -XPUT 'localhost:9200/customers?&pretty'
	 2.curl -XPUT 'localhost:9200/products?&pretty'
	 
2. Display list of indices(index) :-
    GET method is used to display indices in ELS:-
	 curl -XGET 'localhost:9200/_cat/indices?v&pretty'
	  
3. Add document in index :
    PUT method is used to create documents in index. 
     1.curl -XPUT 'localhost:9200/customers/vendors/2?pretty' -d'{"name":"sobha","age":21,"gender":"female","email":"sobha@gmail.com","phone":"123123123","street":"durg","city":"cg","state":"cg"}' -H 'Content-Type:application/json'
     2.curl -XPUT 'localhost:9200/customers/vendors/1?pretty' -d'{"name":"Michael Sharpe","age":22,"gender":"male","email":"michaelsharpe@talkalot.com","phone":"+1 (942) 544-2868","street":"858 Bushwick Court","city":"Dorneyville","state":"American Samoa, 3711"}' -H 'Content-Type: application/json'

4.Display complete Document:-
    GET command is used to display document:-
      1.curl -XGET 'localhost:9200/customers/vendors/1?pretty'
      2.curl -XGET 'localhost:9200/customers/vendors/2?pretty'

5.Display partial document:-
   By specifying fields name from we can retrieve only specified field:-
    curl -XGET 'localhost:9200/customers/vendors/1?pretty&_source=name,email' 

6.Update Document:-
   PUT method is used to update whole document in given index. 
    curl -XPUT 'localhost:9200/customers/vendors/1?pretty' -d'{"name":"priya","age":23,"gender":"female","email":"priya@gmail.com","phone":"215672131","street":"173 Tompkins Place","city":"Epworth","state":"Puerto Rico, 2262"}' -H 'Content-Type: application/json'

7.Update partial document:-
   In order to update document fields we have to specify JSON with key value "doc":-
    curl -XPOST 'localhost:9200/customers/vendors/1/_update?pretty' -d'{
>   "doc": {
>      "age": "24",
>      "city":"Wyano"
>   }
> }
> ' -H 'Content-Type: application/json'

8.Delete document:
	DELETE method is used to delete document:-
	curl -XDELETE 'localhost:9200/customers/vendors/1?pretty'
	
9.Delete Index : -
     curl -XDELETE 'localhost:9200/products?pretty'
	 
10.List all docs in index:-
     curl -X GET 'http://localhost:9200/customers/_search'
	
11.Query using URL parameters:-
	curl -X GET http://localhost:9200/customers/_search?q=name:sobha
	
12. Query with JSON:-
	

Bulk Operation in ES:-
	curl -XPUT 'localhost:9200/products?&pretty'
	
1.Create document in index products with type name shoes:-
	curl -XPUT 'localhost:9200/products/shoes/1?pretty' -d'
> {
>   "name": "Nike",
>   "size": 8,
>   "color": "white"
> }
> ' -H 'Content-Type: application/json'

2.Multiple documents retrieval :-
Elasticsearch allows to retrieve multiple documents using "_mget" API. 
 1. curl -XGET 'localhost:9200/_mget?pretty' -d'
{
> "docs" :[
> {
> "_index" : "customers",
> "_type" : "vendors",
> "_id" : "2"
> },
> {
> "_index" : "products",
> "_type" : "shoes",
> "_id" : "1"
> }
> ]
> }' -H 'Content-Type:application/json'

2.GET /_mget
{
  
 "docs": [
    {
      "_index": "customers",
      "_id": "1"
    },
    {
      "_index": "customers",
      "_id": "2"
    }
  ]
}

3.Retrieve multiple documents with Index passed in service URL :
 curl -XGET 'localhost:9200/customers/_mget?pretty&_source=name,email' -d'       
> {
>     "docs" : [
>         {
> 
>             "_type" : "vendors",
>             "_id" : "1"
>         },
>         {
> 
>             "_type" : "vendors",
>             "_id" : "2"
>         }
>     ]
> }' -H 'Content-Type: application/json'

4.Bulk operation to Create, Update and Delete:-
  curl -XPOST 'localhost:9200/products/shoes/_bulk?pretty' -H 'Content-Type: application/json' -d'
> { "index" : {"_id" : "3" } }
> { "name": "Reef","size": 6,"color": "black" }
> { "index" : {"_id" : "4" } }
> { "name": "PFFlys","size": 7,"color": "Red" }
> { "delete" : {"_id" : "2" } }
> { "create" : {"_id" : "5" } }
> { "name": "Heelys","size": 11,"color": "black" }
> { "update" : {"_id" : "1"} }
> { "doc" : {"color" : "Green"} }
> '

Setting up the number of shards :-
setting up the number of shards is a static operation:-specify it when you are creating an index any change after that require complete reindexing of data.
setting up number of replicas is a dynamic operation :- changes done at any time after index creation also.
curl -XPUT 'localhost:9200/student?pretty' -H 'Content-Type: application/json' -d '
> {
> "settings":{
>     "number_of_shards":2,
>     "number_of_replicas":1
>   }
> }'

Using Kibana CURD Operations:-
JSON data set into Elasticsearch:-
curl -H 'Content-Type: application/x-ndjson' -XPOST 'localhost:9200/bank/account/_bulk?pretty' --data-binary @accounts.json

1.Return all accounts from state UT.
  GET /bank/_search?q=state:UT.

2.Return all accounts from UT or CA.
   GET /bank/_search?q=state:UT OR CA

3.Return all accounts from state TN and from female clients.
   GET /bank/_search?q=state:TN AND gender:F

4.Return all accounts from people between 20 and 25 years old.
   GET /bank/_search?q=age:(>=20 AND <=25)
   
Wildcard Query:-
Using wildcard queries we can search for items without knowing the exact spelling.
 GET /customers/vendors/_search
{
  "query": { 
    "wildcard": { 
      "name": "so??a" 
    }
  }
}

If we don't know the exact character length then we can run the following query:
GET /customers/vendors/_search
{
  "query": { 
    "wildcard": { 
      "name": "s*a" 
    }
  }
}

Boolean Query:-
The boolean query is used to search the results on the basis of joining them with 'or', 'and', 'not' conditions.
GET /customers/vendors/_search
{
  "query": { 
     "bool": {
    "must": [
      {
        "match": {
          "name": "sobha"
        }
      }
    ],     
    "must_not": [
        {
          "match": {
            "street": "raipur"
          }
        }
      ]
    }
  }
}
it will return the documents where both of these items are matched.

Prefix Query:-
GET /_search
{
  "query": {
    "prefix": {
      "name": {
        "value": "so"
      }
    }
  }
}
or 
GET /_search
{
  "query": {
    "prefix" : { "name" : "so" }
  }
}

GET /customers/vendors/_search
{
  "query": { 
    "bool": {
    "should": [
      {
        "match": {
          "name": "sobha"
        }
      }, {
          "match": {
            "city": "cg"
          }
        }
    ]
    }
  }
}

get all the indexes:-
curl -XGET 'http://localhost:9200/_mapping?pretty=true'

Coping one index data to another index:-
 POST /_reindex
{
  "source": {
    "index": "customers"
  },
  "dest": {
    "index": "copied_customers"
  }
}

after coping veryfieg data in kibana:-

GET copied_customers/_search
{
  "query": 
    {
      "match_all":{}
    }
}

Aggregation:-
GET copied_customers/_search
{
  "aggs": {
    "my-agg-name": {
      "terms": {
        "field": "my-field"
      }
    }
  }
}

1. Analyzing Process:-
Character filters:-
 remove the html tags from a text:-
 curl -XPOST 'localhost:9200/_analyze?pretty' -H 'Content-Type: application/json' -d '{
>   "tokenizer": "standard",
>   "char_filter": [
>     "html_strip"
>   ],
>   "

Tokenizer:-
 The input text after its transformation from the Character filter is passed to the tokeniser.
 curl -XPOST 'localhost:9200/_analyze?pretty' -H 'Content-Type: application/json' -d '{
> "tokenizer":"standard",
> "text":"the Auto-generation is success"
> }'

Token filter:-
 curl -XPOST 'localhost:9200/_analyze?pretty' -H 'Content-Type: application/json' -d'{
>   "tokenizer": "standard",
>   "filter": [
>     "lowercase"
>   ],
>   "text": "The Auto-generation is a success"
> }'

2.Analyzers:-
 The combination of these three components (character filters,tokenizers and token filters) are called as Analyzers.
 Standard Analyzer:-is a combination of a standard tokenizer and two token filters (standard token filter, lowercase and stop token filter). 
 PUT standardindex
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "standard",
          "max_token_length": 5
        }
      }
    }
  }
}
POST standardIndex/_analyze
{
  "analyzer": "my_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]

 simple analyzer:-
	POST _analyze
	{
	  "analyzer": "simple",
	  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
	}
   [ the, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]
   	It consists of: Lowercase Tokenizer
	PUT /simpleindex
	{
	  "settings": {
		"analysis": {
		  "analyzer": {
			"my_custom_simple_analyzer": {
			  "tokenizer": "lowercase",
			  "filter": [                          
			  ]
			}
		  }
		}
	  }
	}
	
 whitespace analyzer :-
	 POST _analyze
	{
	  "analyzer": "whitespace",
	  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
	}
	[ The, 2, QUICK, Brown-Foxes, jumped, over, the, lazy, dog's, bone. ]
	It consists of: whitespace tokenizer
	PUT /whitespace_example
 {
  "settings": {
    "analysis": {
      "analyzer": {
        "rebuilt_whitespace": {
          "tokenizer": "whitespace",
          "filter": [         
          ]
        }
      }
    }
  }
}
Stop analyzer:-
 The stop analyzer is the same as the simple analyzer but adds support for removing stop words. 
 POST _analyze
{
  "analyzer": "stop",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
[ quick, brown, foxes, jumped, over, lazy, dog, s, bone ]

The stop analyzer accepts the following parameters: stopwords,stopwords_path
PUT stop-index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_stop_analyzer": {
          "type": "stop",
          "stopwords": ["the", "over"]
        }
      }
    }
  }
}

POST stop-index/_analyze
{
  "analyzer": "my_stop_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
[ quick, brown, foxes, jumped, lazy, dog, s, bone ]

It consists of:
 Lower Case Tokenizer & Stop Token Filter
 PUT /stop_example
{
  "settings": {
    "analysis": {
      "filter": {
        "english_stop": {
          "type":       "stop",
          "stopwords":  "_english_" 
        }
      },
      "analyzer": {
        "rebuilt_stop": {
          "tokenizer": "lowercase",
          "filter": [
            "english_stop"          
          ]
        }
      }
    }
  }
}

 keyword analyzer:-returns the entire input string as a single token.
 POST _analyze
{
  "analyzer": "keyword",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
[ The 2 QUICK Brown-Foxes jumped over the lazy dog's bone. ]
 It consists of:-Keyword Tokenizer
 
 pattern analyzer :-
  The pattern analyzer uses a regular expression to split the text into terms. 
  POST _analyze
{
  "analyzer": "pattern",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]
PUT my-index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_email_analyzer": {
          "type":      "pattern",
          "pattern":   "\\W|_", 
          "lowercase": true
        }
      }
    }
  }
}
POST my-index/_analyze
{
  "analyzer": "my_email_analyzer",
  "text": "John_Smith@foo-bar.com"
}
[ john, smith, foo, bar, com ]
The pattern anlayzer consists of:
 Pattern Tokenizer
 Lower Case Token Filter
 Stop Token Filter
 
 Language Analyzers:-
  example:
  arabic analyzer:-
  PUT /arabic_example
{
  "settings": {
    "analysis": {
      "filter": {
        "arabic_stop": {
          "type":       "stop",
          "stopwords":  "_arabic_" 
        },
      },
      "analyzer": {
        "rebuilt_arabic": {
          "tokenizer":  "standard",
          "filter": [
            "lowercase",
            "decimal_digit",
            "arabic_keywords",
          ]
        }
      }
    }
  }
}
 fingerprint analyzer :-Input text is lowercased, normalized to remove extended characters, sorted, deduplicated and concatenated into a single token. 
 
  POST _analyze
{
  "analyzer": "fingerprint",
  "text": "Yes yes, Gödel said this sentence is consistent and."
}
[ and consistent godel is said sentence this yes ]
PUT my-index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_fingerprint_analyzer": {
          "type": "fingerprint",
          "stopwords": "_english_"
        }
      }
    }
  }
}

POST my-index/_analyze
{
  "analyzer": "my_fingerprint_analyzer",
  "text": "Yes yes, Gödel said this sentence is consistent and."
}
 [ consistent godel said sentence yes ]
 The fingerprint tokenizer consists of:
 Standard Tokenizer
 
3. Analysis Phases:-
  Index time analysis:-
 curl -XPOST localhost:9200/students/class/1 -d '{
   "text": "My name is Arun"
}'
Elasticsearch applies the default analyzer “Standard analyzer” on this.

curl -XPOST 'localhost:9200/_analyze?pretty' -H 'Content-Type: application/json' -d'{
   "analyzer": "standard",
   "text": "My name is Arun"
}'
token :-“my”,”name”,”is”,”arun”

Freezes an index:-indices that are still open — keeping them searchable — but do not occupy heap
When we freeze an index, it becomes read-only and its transient data structures are dropped from memory.
but only one frozen shard is executing per node at the same time. 
 POST /index/_freeze
 
 POST /sampledata/_doc
{
    "name": "Jane",
    "lastname": "Doe"
}
POST /sampledata/_doc
{
    "name": "John",
    "lastname": "Doe"
}
GET /sampledata/_search
POST /sampledata/_forcemerge?max_num_segments=1
POST /sampledata/_freeze

GET /sampledata/_search?ignore_throttled=false
{
 "query": {
   "match": {
     "name": "jane"
   }
 }
}

 Pipeline converter:-
 PUT _ingest/pipeline/pipeline-id
 {
  "description": "converts the content of the id field to an integer",
  "processors" : [
	{
	  "convert" : {
		"field" : "id",
		"type": "integer"
	  }
	}
  ]
 }
 
 Get distinct values from a field in ElasticSearch:-
 aggregation to get distinct values :-
GET /customers/vendors/_search
{
"aggs": {
    "distinct_age": {
      "terms": {
        "field": "age"
      }
    }
  }
}

GET /customers/vendors/_search
{
   "query" : {
        "bool" : {
               "should" : [ 
                  { "term" : { "name" : "sobha" } }
               ]
         }
    },
"aggs": {
    "distinct_age": {
      "terms": {
        "field": "age"
      }
    }
  }
}

Elasticsearch fire SQL query:-
PUT /library/book/_bulk?refresh
{"index":{"_id": "Leviathan Wakes"}}
{"name": "Leviathan Wakes", "author": "James S.A. Corey", "release_date": "2011-06-02", "page_count": 561}
{"index":{"_id": "Hyperion"}}
{"name": "Hyperion", "author": "Dan Simmons", "release_date": "1989-05-26", "page_count": 482}
{"index":{"_id": "Dune"}}
{"name": "Dune", "author": "Frank Herbert", "release_date": "1965-06-01", "page_count": 604}

execute SQL using the SQL REST API(SQL REST API accepts SQL in a JSON document, executes it, and returns the results.):-
txt format:-
POST /_sql?format=txt
{
  "query": "SELECT * FROM library WHERE release_date < '2000-01-01'"
}
csv format:- 
POST /_sql?format=csv
{
  "query": "SELECT * FROM library ORDER BY page_count DESC",
  "fetch_size": 5
}
json format:-
POST /_sql?format=json
{
  "query": "SELECT * FROM library ORDER BY page_count DESC",
  "fetch_size": 5
}

Filtring using Elasticsearch query DSL:-
POST /_sql?format=txt
{
  "query": "SELECT * FROM library ORDER BY page_count DESC",
  "filter": {
    "range": {
      "page_count": {
        "gte" : 100,
        "lte" : 500
      }
    }
  },
  "fetch_size": 5
}

columnar fashion:- one row represents all the values of a certain column.
POST /_sql?format=json
{
  "query": "SELECT * FROM library ORDER BY page_count DESC",
  "fetch_size": 5,
  "columnar": true
}

in kibana:-
GET /_sql
{
  "query": """SELECT * FROM library WHERE release_date < '2000-01-01'"""
}
Partitioning implementations in elasticsearch:-
PUT my_index_partition
{
  "settings": {
     "index": { 
       "number_of_shards": 10, 
        "routing_partition_size": 2,
        "sort.field": ["id"],
        "sort.order": ["asc"] 
        }
    }
}
 
Match Query:-
 match queries accept text/numerics/dates, analyzes them, and constructs a query.
GET /_search
{
    "query": {
        "match" : {
            "name" : "sobha"
        }
    }
}
 match_phrase query analyzes:-analyzes the text and creates a phrase query
 GET /_search
{
    "query": {
        "match_phrase" : {
            "street" : "durg cg"
        }
    }
}
